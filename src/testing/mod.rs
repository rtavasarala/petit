//! Testing utilities for users of the Petit library.
//!
//! This module provides helpers for testing task orchestration:
//!
//! - [`MockTaskContext`]: A context that provides inputs and captures outputs
//! - [`TestHarness`]: Runs DAGs with in-memory storage and timeline tracking
//! - [`FailingTask`]: A task helper that fails N times then succeeds
//! - [`Timeline`]: Records execution events for verification

use async_trait::async_trait;
use serde_json::Value;
use std::collections::HashMap;
use std::sync::atomic::{AtomicU32, Ordering};
use std::sync::{Arc, RwLock};
use std::time::{Duration, Instant};
use tokio::sync::Mutex;

use crate::core::context::TaskContext;
use crate::core::dag::Dag;
use crate::core::retry::RetryPolicy;
use crate::core::task::{Task, TaskError};
use crate::core::types::TaskId;
use crate::execution::{DagExecutor, DagResult};

/// A mock task context for testing.
///
/// Provides the ability to:
/// - Pre-set input values that tasks can read
/// - Capture outputs written by tasks
///
/// # Example
///
/// ```
/// use petit::testing::MockTaskContext;
///
/// let mut mock = MockTaskContext::new("my_task");
/// mock.set_input("upstream.value", 42);
///
/// // Run your task with mock.as_context()
/// let ctx = mock.as_context();
/// // After running a task that writes to "my_task.output":
/// // let output: i32 = mock.get_output("my_task.output").unwrap();
///
/// // Verify the input was set
/// let input: i32 = ctx.inputs.get("upstream.value").unwrap();
/// assert_eq!(input, 42);
/// ```
pub struct MockTaskContext {
    task_id: TaskId,
    store: Arc<RwLock<HashMap<String, Value>>>,
    config: Arc<HashMap<String, Value>>,
}

impl MockTaskContext {
    /// Create a new mock context for a task.
    pub fn new(task_id: impl Into<String>) -> Self {
        Self {
            task_id: TaskId::new(task_id),
            store: Arc::new(RwLock::new(HashMap::new())),
            config: Arc::new(HashMap::new()),
        }
    }

    /// Create a mock context with configuration values.
    pub fn with_config(task_id: impl Into<String>, config: HashMap<String, Value>) -> Self {
        Self {
            task_id: TaskId::new(task_id),
            store: Arc::new(RwLock::new(HashMap::new())),
            config: Arc::new(config),
        }
    }

    /// Set an input value that the task can read.
    ///
    /// Use the full namespaced key (e.g., "upstream_task.value").
    pub fn set_input<T: serde::Serialize>(&mut self, key: impl Into<String>, value: T) {
        let key = key.into();
        let json_value = serde_json::to_value(value).expect("failed to serialize input value");
        self.store
            .write()
            .expect("lock poisoned")
            .insert(key, json_value);
    }

    /// Get an output value written by the task.
    ///
    /// Returns `None` if the key doesn't exist or deserialization fails.
    pub fn get_output<T: serde::de::DeserializeOwned>(&self, key: &str) -> Option<T> {
        self.store
            .read()
            .ok()?
            .get(key)
            .and_then(|v| serde_json::from_value(v.clone()).ok())
    }

    /// Check if an output key was written.
    pub fn has_output(&self, key: &str) -> bool {
        self.store
            .read()
            .map(|s| s.contains_key(key))
            .unwrap_or(false)
    }

    /// Get the underlying store as a HashMap.
    pub fn store(&self) -> HashMap<String, Value> {
        self.store.read().map(|s| s.clone()).unwrap_or_default()
    }

    /// Convert to a TaskContext for use in task execution.
    pub fn as_context(&self) -> TaskContext {
        TaskContext::new(
            self.store.clone(),
            self.task_id.clone(),
            self.config.clone(),
        )
    }
}

/// A task that fails a configurable number of times before succeeding.
///
/// Useful for testing retry logic and error handling.
///
/// This task is safe for concurrent execution - the failure counting is
/// protected by a mutex to ensure deterministic behavior.
///
/// # Example
///
/// ```
/// use petit::testing::FailingTask;
///
/// // Fails 2 times, then succeeds on the 3rd attempt
/// let task = FailingTask::new("flaky_task", 2);
/// ```
pub struct FailingTask {
    name: String,
    /// Mutex protecting failure state to prevent race conditions under concurrent execution.
    state: Mutex<FailingTaskState>,
    total_failures: u32,
    error_message: String,
    retry_policy: RetryPolicy,
}

/// Internal state for FailingTask, protected by a mutex.
struct FailingTaskState {
    failures_remaining: u32,
    call_count: u32,
}

impl FailingTask {
    /// Create a task that fails `fail_count` times then succeeds.
    pub fn new(name: impl Into<String>, fail_count: u32) -> Self {
        Self {
            name: name.into(),
            state: Mutex::new(FailingTaskState {
                failures_remaining: fail_count,
                call_count: 0,
            }),
            total_failures: fail_count,
            error_message: "intentional test failure".to_string(),
            retry_policy: RetryPolicy::none(),
        }
    }

    /// Create a task that fails with a custom error message.
    pub fn with_error(
        name: impl Into<String>,
        fail_count: u32,
        message: impl Into<String>,
    ) -> Self {
        Self {
            name: name.into(),
            state: Mutex::new(FailingTaskState {
                failures_remaining: fail_count,
                call_count: 0,
            }),
            total_failures: fail_count,
            error_message: message.into(),
            retry_policy: RetryPolicy::none(),
        }
    }

    /// Set a retry policy for this task.
    pub fn with_retry_policy(mut self, policy: RetryPolicy) -> Self {
        self.retry_policy = policy;
        self
    }

    /// Get the number of failures remaining before success.
    ///
    /// Note: This is an async method because it acquires a lock.
    pub async fn failures_remaining(&self) -> u32 {
        self.state.lock().await.failures_remaining
    }

    /// Get the number of times this task has been called.
    ///
    /// Note: This is an async method because it acquires a lock.
    pub async fn call_count(&self) -> u32 {
        self.state.lock().await.call_count
    }

    /// Reset the failure counter for reuse.
    ///
    /// Note: This is an async method because it acquires a lock.
    pub async fn reset(&self) {
        let mut state = self.state.lock().await;
        state.failures_remaining = self.total_failures;
        state.call_count = 0;
    }
}

#[async_trait]
impl Task for FailingTask {
    fn name(&self) -> &str {
        &self.name
    }

    async fn execute(&self, ctx: &mut TaskContext) -> Result<(), TaskError> {
        // Lock the state to ensure atomic check-and-decrement under concurrent execution
        let mut state = self.state.lock().await;

        // Track call count
        state.call_count += 1;

        if state.failures_remaining > 0 {
            // Decrement and fail
            state.failures_remaining -= 1;
            Err(TaskError::ExecutionFailed(self.error_message.clone()))
        } else {
            // No failures remaining, succeed
            ctx.outputs.set("success", true)?;
            Ok(())
        }
    }

    fn retry_policy(&self) -> RetryPolicy {
        self.retry_policy.clone()
    }
}

/// An entry in the execution timeline.
#[derive(Debug, Clone)]
pub struct TimelineEntry {
    /// The task ID.
    pub task_id: TaskId,
    /// The type of event.
    pub event_type: TimelineEventType,
    /// When this event occurred.
    pub timestamp: Instant,
    /// Optional duration (for completed events).
    pub duration: Option<Duration>,
    /// Optional error message (for failed events).
    pub error: Option<String>,
}

/// Types of events in the timeline.
#[derive(Debug, Clone, PartialEq, Eq)]
pub enum TimelineEventType {
    /// Task started executing.
    Started,
    /// Task completed successfully.
    Completed,
    /// Task failed.
    Failed,
    /// Task is being retried.
    Retrying { attempt: u32, max_attempts: u32 },
}

/// Shared execution tracker for recording task execution order.
///
/// Tasks can use this to record when they start/complete, enabling
/// timeline verification in tests.
#[derive(Clone)]
pub struct ExecutionTracker {
    entries: Arc<Mutex<Vec<TimelineEntry>>>,
    order_counter: Arc<AtomicU32>,
}

impl ExecutionTracker {
    /// Create a new execution tracker.
    pub fn new() -> Self {
        Self {
            entries: Arc::new(Mutex::new(Vec::new())),
            order_counter: Arc::new(AtomicU32::new(0)),
        }
    }

    /// Record that a task started.
    pub async fn record_start(&self, task_id: &TaskId) {
        let _order = self.order_counter.fetch_add(1, Ordering::SeqCst);
        self.entries.lock().await.push(TimelineEntry {
            task_id: task_id.clone(),
            event_type: TimelineEventType::Started,
            timestamp: Instant::now(),
            duration: None,
            error: None,
        });
    }

    /// Record that a task completed successfully.
    pub async fn record_complete(&self, task_id: &TaskId, duration: Duration) {
        self.entries.lock().await.push(TimelineEntry {
            task_id: task_id.clone(),
            event_type: TimelineEventType::Completed,
            timestamp: Instant::now(),
            duration: Some(duration),
            error: None,
        });
    }

    /// Record that a task failed.
    pub async fn record_failure(&self, task_id: &TaskId, error: String) {
        self.entries.lock().await.push(TimelineEntry {
            task_id: task_id.clone(),
            event_type: TimelineEventType::Failed,
            timestamp: Instant::now(),
            duration: None,
            error: Some(error),
        });
    }
}

impl Default for ExecutionTracker {
    fn default() -> Self {
        Self::new()
    }
}

/// Records events for timeline verification.
///
/// Use with `TestHarness` to verify execution order. This works with
/// `TrackedTask` wrappers that record their execution to the timeline.
pub struct Timeline {
    entries: Arc<Mutex<Vec<TimelineEntry>>>,
}

impl Timeline {
    /// Create a new timeline recorder.
    pub fn new() -> Self {
        Self {
            entries: Arc::new(Mutex::new(Vec::new())),
        }
    }

    /// Create a timeline from an execution tracker.
    pub fn from_tracker(tracker: &ExecutionTracker) -> Self {
        Self {
            entries: tracker.entries.clone(),
        }
    }

    /// Get all recorded entries.
    pub async fn entries(&self) -> Vec<TimelineEntry> {
        self.entries.lock().await.clone()
    }

    /// Get entries for a specific task.
    pub async fn entries_for_task(&self, task_id: &TaskId) -> Vec<TimelineEntry> {
        self.entries
            .lock()
            .await
            .iter()
            .filter(|e| &e.task_id == task_id)
            .cloned()
            .collect()
    }

    /// Get the order in which tasks started.
    pub async fn start_order(&self) -> Vec<TaskId> {
        self.entries
            .lock()
            .await
            .iter()
            .filter(|e| e.event_type == TimelineEventType::Started)
            .map(|e| e.task_id.clone())
            .collect()
    }

    /// Get the order in which tasks completed (successfully or failed).
    pub async fn completion_order(&self) -> Vec<TaskId> {
        self.entries
            .lock()
            .await
            .iter()
            .filter(|e| {
                matches!(
                    e.event_type,
                    TimelineEventType::Completed | TimelineEventType::Failed
                )
            })
            .map(|e| e.task_id.clone())
            .collect()
    }

    /// Verify that tasks started in the given order.
    ///
    /// Returns `Ok(())` if order matches, `Err` with details otherwise.
    pub async fn assert_start_order(&self, expected: &[&str]) -> Result<(), String> {
        let actual = self.start_order().await;
        let expected: Vec<TaskId> = expected.iter().map(|s| TaskId::new(*s)).collect();

        if actual == expected {
            Ok(())
        } else {
            Err(format!(
                "Start order mismatch.\nExpected: {:?}\nActual: {:?}",
                expected.iter().map(|t| t.as_str()).collect::<Vec<_>>(),
                actual.iter().map(|t| t.as_str()).collect::<Vec<_>>()
            ))
        }
    }

    /// Verify that a task started before another.
    pub async fn assert_started_before(&self, first: &str, second: &str) -> Result<(), String> {
        let order = self.start_order().await;
        let first_idx = order.iter().position(|t| t.as_str() == first);
        let second_idx = order.iter().position(|t| t.as_str() == second);

        match (first_idx, second_idx) {
            (Some(f), Some(s)) if f < s => Ok(()),
            (Some(f), Some(s)) => Err(format!(
                "Task '{}' started at index {}, but '{}' started earlier at index {}",
                first, f, second, s
            )),
            (None, _) => Err(format!("Task '{}' not found in timeline", first)),
            (_, None) => Err(format!("Task '{}' not found in timeline", second)),
        }
    }

    /// Check if tasks ran concurrently (overlapping time windows).
    pub async fn ran_concurrently(&self, task_a: &str, task_b: &str) -> bool {
        let entries = self.entries.lock().await;

        // Find start/end for task_a
        let a_start = entries
            .iter()
            .find(|e| e.task_id.as_str() == task_a && e.event_type == TimelineEventType::Started)
            .map(|e| e.timestamp);
        let a_end = entries
            .iter()
            .find(|e| {
                e.task_id.as_str() == task_a
                    && matches!(
                        e.event_type,
                        TimelineEventType::Completed | TimelineEventType::Failed
                    )
            })
            .map(|e| e.timestamp);

        // Find start/end for task_b
        let b_start = entries
            .iter()
            .find(|e| e.task_id.as_str() == task_b && e.event_type == TimelineEventType::Started)
            .map(|e| e.timestamp);
        let b_end = entries
            .iter()
            .find(|e| {
                e.task_id.as_str() == task_b
                    && matches!(
                        e.event_type,
                        TimelineEventType::Completed | TimelineEventType::Failed
                    )
            })
            .map(|e| e.timestamp);

        // Check for overlap: A starts before B ends AND B starts before A ends
        match (a_start, a_end, b_start, b_end) {
            (Some(as_), Some(ae), Some(bs), Some(be)) => as_ < be && bs < ae,
            _ => false,
        }
    }
}

impl Default for Timeline {
    fn default() -> Self {
        Self::new()
    }
}

/// A task wrapper that records execution to an ExecutionTracker.
///
/// Wrap any task with this to enable timeline tracking.
pub struct TrackedTask<T: Task> {
    inner: T,
    tracker: ExecutionTracker,
}

impl<T: Task> TrackedTask<T> {
    /// Create a tracked wrapper around a task.
    pub fn new(task: T, tracker: ExecutionTracker) -> Self {
        Self {
            inner: task,
            tracker,
        }
    }
}

#[async_trait]
impl<T: Task> Task for TrackedTask<T> {
    fn name(&self) -> &str {
        self.inner.name()
    }

    async fn execute(&self, ctx: &mut TaskContext) -> Result<(), TaskError> {
        let task_id = TaskId::new(self.inner.name());
        let start = Instant::now();

        self.tracker.record_start(&task_id).await;

        let result = self.inner.execute(ctx).await;

        match &result {
            Ok(()) => {
                self.tracker
                    .record_complete(&task_id, start.elapsed())
                    .await;
            }
            Err(e) => {
                self.tracker.record_failure(&task_id, e.to_string()).await;
            }
        }

        result
    }

    fn retry_policy(&self) -> RetryPolicy {
        self.inner.retry_policy()
    }

    fn description(&self) -> Option<&str> {
        self.inner.description()
    }
}

/// Configuration for failure injection in TestHarness.
#[derive(Clone, Default)]
pub struct FailureInjection {
    /// Tasks that should always fail.
    pub always_fail: Vec<String>,
    /// Tasks that should fail with specific error messages.
    pub fail_with_message: HashMap<String, String>,
    /// Tasks that should fail N times then succeed.
    pub fail_n_times: HashMap<String, u32>,
}

impl FailureInjection {
    /// Create an empty failure injection config.
    pub fn new() -> Self {
        Self::default()
    }

    /// Configure a task to always fail.
    pub fn always_fail(mut self, task_id: impl Into<String>) -> Self {
        self.always_fail.push(task_id.into());
        self
    }

    /// Configure a task to fail with a specific message.
    pub fn fail_with(mut self, task_id: impl Into<String>, message: impl Into<String>) -> Self {
        self.fail_with_message
            .insert(task_id.into(), message.into());
        self
    }

    /// Configure a task to fail N times then succeed.
    pub fn fail_times(mut self, task_id: impl Into<String>, times: u32) -> Self {
        self.fail_n_times.insert(task_id.into(), times);
        self
    }
}

/// A test harness for running DAGs with in-memory storage.
///
/// Provides:
/// - Automatic timeline tracking (when using TrackedTask wrappers)
/// - Execution with configurable concurrency
/// - Failure injection configuration
/// - Assertions on execution results
///
/// # Example
///
/// ```ignore
/// use petit::testing::{TestHarness, TrackedTask, ExecutionTracker};
/// use petit::DagBuilder;
///
/// let tracker = ExecutionTracker::new();
/// let harness = TestHarness::new()
///     .with_tracker(tracker.clone())
///     .with_concurrency(2);
///
/// let dag = DagBuilder::new("test", "Test DAG")
///     .add_task(Arc::new(TrackedTask::new(my_task, tracker.clone())))
///     .build()?;
///
/// let result = harness.execute(&dag).await;
///
/// assert!(result.dag_result.success);
/// harness.timeline().assert_start_order(&["task_a", "task_b"]).await?;
/// ```
pub struct TestHarness {
    executor: DagExecutor,
    tracker: ExecutionTracker,
    failure_injection: FailureInjection,
    initial_context: HashMap<String, Value>,
}

/// Result from TestHarness execution.
pub struct TestResult {
    /// The DAG execution result.
    pub dag_result: DagResult,
    /// The final context after execution.
    pub context: HashMap<String, Value>,
}

impl TestHarness {
    /// Create a new test harness.
    pub fn new() -> Self {
        Self {
            executor: DagExecutor::with_concurrency(4),
            tracker: ExecutionTracker::new(),
            failure_injection: FailureInjection::default(),
            initial_context: HashMap::new(),
        }
    }

    /// Set the concurrency limit for task execution.
    pub fn with_concurrency(mut self, max_concurrency: usize) -> Self {
        self.executor = DagExecutor::with_concurrency(max_concurrency);
        self
    }

    /// Set a custom execution tracker for timeline recording.
    pub fn with_tracker(mut self, tracker: ExecutionTracker) -> Self {
        self.tracker = tracker;
        self
    }

    /// Configure failure injection for specific tasks.
    pub fn with_failures(mut self, injection: FailureInjection) -> Self {
        self.failure_injection = injection;
        self
    }

    /// Set initial context values available to all tasks.
    pub fn with_context<T: serde::Serialize>(mut self, key: impl Into<String>, value: T) -> Self {
        let json_value = serde_json::to_value(value).expect("failed to serialize context value");
        self.initial_context.insert(key.into(), json_value);
        self
    }

    /// Get the execution tracker for this harness.
    pub fn tracker(&self) -> &ExecutionTracker {
        &self.tracker
    }

    /// Get a timeline view of the execution.
    pub fn timeline(&self) -> Timeline {
        Timeline::from_tracker(&self.tracker)
    }

    /// Execute a DAG and return the result.
    pub async fn execute(&self, dag: &Dag) -> TestResult {
        // Create context with initial values
        let store = Arc::new(RwLock::new(self.initial_context.clone()));
        let config = Arc::new(HashMap::new());
        let mut ctx = TaskContext::new(store.clone(), TaskId::new("harness"), config);

        // Execute the DAG
        let dag_result = self.executor.execute(dag, &mut ctx).await;

        // Extract final context
        let context = store.read().map(|s| s.clone()).unwrap_or_default();

        TestResult {
            dag_result,
            context,
        }
    }

    /// Execute and assert success.
    pub async fn execute_and_assert_success(&self, dag: &Dag) -> TestResult {
        let result = self.execute(dag).await;
        assert!(
            result.dag_result.success,
            "Expected DAG to succeed, but it failed. Failed tasks: {:?}",
            result.dag_result.failed_tasks
        );
        result
    }

    /// Execute and assert failure.
    pub async fn execute_and_assert_failure(&self, dag: &Dag) -> TestResult {
        let result = self.execute(dag).await;
        assert!(
            !result.dag_result.success,
            "Expected DAG to fail, but it succeeded"
        );
        result
    }
}

impl Default for TestHarness {
    fn default() -> Self {
        Self::new()
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use crate::core::dag::DagBuilder;
    use crate::core::retry::RetryPolicy;
    use std::time::Duration;

    // Simple task for testing
    struct SimpleTask {
        name: String,
    }

    impl SimpleTask {
        fn new(name: &str) -> Arc<Self> {
            Arc::new(Self {
                name: name.to_string(),
            })
        }
    }

    #[async_trait]
    impl Task for SimpleTask {
        fn name(&self) -> &str {
            &self.name
        }

        async fn execute(&self, ctx: &mut TaskContext) -> Result<(), TaskError> {
            ctx.outputs.set("executed", true)?;
            Ok(())
        }
    }

    // Task that reads and transforms
    struct TransformTask {
        name: String,
        input_key: String,
    }

    impl TransformTask {
        fn new(name: &str, input_key: &str) -> Arc<Self> {
            Arc::new(Self {
                name: name.to_string(),
                input_key: input_key.to_string(),
            })
        }
    }

    #[async_trait]
    impl Task for TransformTask {
        fn name(&self) -> &str {
            &self.name
        }

        async fn execute(&self, ctx: &mut TaskContext) -> Result<(), TaskError> {
            let input: i32 = ctx.inputs.get(&self.input_key)?;
            ctx.outputs.set("result", input * 2)?;
            Ok(())
        }
    }

    // ==========================================================================
    // MockTaskContext Tests
    // ==========================================================================

    #[test]
    fn test_mock_context_provides_inputs() {
        let mut mock = MockTaskContext::new("test_task");
        mock.set_input("upstream.value", 42);
        mock.set_input("upstream.name", "hello");

        let ctx = mock.as_context();
        let value: i32 = ctx.inputs.get("upstream.value").unwrap();
        let name: String = ctx.inputs.get("upstream.name").unwrap();

        assert_eq!(value, 42);
        assert_eq!(name, "hello");
    }

    #[tokio::test]
    async fn test_mock_context_captures_outputs() {
        let mock = MockTaskContext::new("writer");
        let ctx = mock.as_context();

        // Simulate task writing output
        ctx.outputs.set("result", 100).unwrap();
        ctx.outputs.set("status", "done").unwrap();

        // Verify outputs through mock
        assert_eq!(mock.get_output::<i32>("writer.result"), Some(100));
        assert_eq!(
            mock.get_output::<String>("writer.status"),
            Some("done".to_string())
        );
    }

    #[test]
    fn test_mock_context_has_output() {
        let mut mock = MockTaskContext::new("task");
        mock.set_input("key", "value");

        assert!(mock.has_output("key"));
        assert!(!mock.has_output("nonexistent"));
    }

    #[test]
    fn test_mock_context_with_config() {
        let mut config = HashMap::new();
        config.insert("batch_size".to_string(), serde_json::json!(100));
        config.insert("timeout".to_string(), serde_json::json!(30));

        let mock = MockTaskContext::with_config("task", config);
        let ctx = mock.as_context();

        let batch_size: i32 = ctx.get_config("batch_size").unwrap();
        let timeout: i32 = ctx.get_config("timeout").unwrap();

        assert_eq!(batch_size, 100);
        assert_eq!(timeout, 30);
    }

    // ==========================================================================
    // FailingTask Tests
    // ==========================================================================

    #[tokio::test]
    async fn test_failing_task_fails_n_times_then_succeeds() {
        let task = FailingTask::new("flaky", 2);
        let mock = MockTaskContext::new("flaky");
        let mut ctx = mock.as_context();

        // First call - fails
        let result1 = task.execute(&mut ctx).await;
        assert!(result1.is_err());
        assert_eq!(task.call_count().await, 1);

        // Second call - fails
        let result2 = task.execute(&mut ctx).await;
        assert!(result2.is_err());
        assert_eq!(task.call_count().await, 2);

        // Third call - succeeds
        let result3 = task.execute(&mut ctx).await;
        assert!(result3.is_ok());
        assert_eq!(task.call_count().await, 3);
    }

    #[tokio::test]
    async fn test_failing_task_with_custom_error() {
        let task = FailingTask::with_error("bad_task", 1, "custom error message");
        let mock = MockTaskContext::new("bad_task");
        let mut ctx = mock.as_context();

        let result = task.execute(&mut ctx).await;
        let err = result.unwrap_err();

        assert!(err.to_string().contains("custom error message"));
    }

    #[tokio::test]
    async fn test_failing_task_reset() {
        let task = FailingTask::new("resettable", 1);
        let mock = MockTaskContext::new("resettable");
        let mut ctx = mock.as_context();

        // Fail once
        let _ = task.execute(&mut ctx).await;

        // Succeed
        let result = task.execute(&mut ctx).await;
        assert!(result.is_ok());

        // Reset and fail again
        task.reset().await;
        let result = task.execute(&mut ctx).await;
        assert!(result.is_err());
    }

    #[test]
    fn test_failing_task_with_retry_policy() {
        let task = FailingTask::new("retry_task", 3)
            .with_retry_policy(RetryPolicy::fixed(5, Duration::from_millis(100)));

        let policy = task.retry_policy();
        assert_eq!(policy.max_attempts, 5);
    }

    // ==========================================================================
    // TestHarness Tests
    // ==========================================================================

    #[tokio::test]
    async fn test_harness_runs_dag_with_in_memory_storage() {
        let harness = TestHarness::new();

        let dag = DagBuilder::new("test", "Test DAG")
            .add_task(SimpleTask::new("task_a"))
            .build()
            .unwrap();

        let result = harness.execute(&dag).await;

        assert!(result.dag_result.success);
        assert_eq!(result.dag_result.completed_count(), 1);
    }

    #[tokio::test]
    async fn test_harness_execute_and_assert_success() {
        let harness = TestHarness::new();

        let dag = DagBuilder::new("test", "Test DAG")
            .add_task(SimpleTask::new("a"))
            .add_task_with_deps(SimpleTask::new("b"), &["a"])
            .build()
            .unwrap();

        let result = harness.execute_and_assert_success(&dag).await;
        assert_eq!(result.dag_result.completed_count(), 2);
    }

    #[tokio::test]
    async fn test_harness_execute_and_assert_failure() {
        let harness = TestHarness::new();

        let dag = DagBuilder::new("test", "Test DAG")
            .add_task(Arc::new(FailingTask::new("failing", 100)) as Arc<dyn Task>)
            .build()
            .unwrap();

        let result = harness.execute_and_assert_failure(&dag).await;
        assert_eq!(result.dag_result.failed_count(), 1);
    }

    #[tokio::test]
    async fn test_harness_with_initial_context() {
        let harness = TestHarness::new().with_context("initial.value", 21);

        let dag = DagBuilder::new("test", "Test DAG")
            .add_task(TransformTask::new("transform", "initial.value"))
            .build()
            .unwrap();

        let result = harness.execute(&dag).await;

        assert!(result.dag_result.success);
        let output: i32 =
            serde_json::from_value(result.context.get("transform.result").unwrap().clone())
                .unwrap();
        assert_eq!(output, 42);
    }

    // ==========================================================================
    // Timeline Tests (using TrackedTask for execution recording)
    // ==========================================================================

    /// Helper to create a tracked task that records to a shared tracker.
    fn tracked<T: Task + 'static>(task: T, tracker: &ExecutionTracker) -> Arc<dyn Task> {
        Arc::new(TrackedTask::new(task, tracker.clone()))
    }

    // Simple task that doesn't use Arc for tracking tests
    struct SimpleTaskInner {
        name: String,
    }

    impl SimpleTaskInner {
        fn new(name: &str) -> Self {
            Self {
                name: name.to_string(),
            }
        }
    }

    #[async_trait]
    impl Task for SimpleTaskInner {
        fn name(&self) -> &str {
            &self.name
        }

        async fn execute(&self, ctx: &mut TaskContext) -> Result<(), TaskError> {
            ctx.outputs.set("executed", true)?;
            Ok(())
        }
    }

    // Slow task inner for tracking tests
    struct SlowTaskInner {
        name: String,
        duration: Duration,
    }

    impl SlowTaskInner {
        fn new(name: &str, duration: Duration) -> Self {
            Self {
                name: name.to_string(),
                duration,
            }
        }
    }

    #[async_trait]
    impl Task for SlowTaskInner {
        fn name(&self) -> &str {
            &self.name
        }

        async fn execute(&self, ctx: &mut TaskContext) -> Result<(), TaskError> {
            tokio::time::sleep(self.duration).await;
            ctx.outputs.set("completed", true)?;
            Ok(())
        }
    }

    #[tokio::test]
    async fn test_timeline_records_execution_order() {
        let tracker = ExecutionTracker::new();
        let harness = TestHarness::new().with_tracker(tracker.clone());

        let dag = DagBuilder::new("timeline_test", "Timeline Test DAG")
            .add_task(tracked(SimpleTaskInner::new("a"), &tracker))
            .add_task_with_deps(tracked(SimpleTaskInner::new("b"), &tracker), &["a"])
            .add_task_with_deps(tracked(SimpleTaskInner::new("c"), &tracker), &["b"])
            .build()
            .unwrap();

        let _ = harness.execute(&dag).await;

        // Verify start order - linear DAG should execute in order
        let start_order = harness.timeline().start_order().await;
        assert_eq!(start_order.len(), 3);
        assert_eq!(start_order[0].as_str(), "a");
        assert_eq!(start_order[1].as_str(), "b");
        assert_eq!(start_order[2].as_str(), "c");
    }

    #[tokio::test]
    async fn test_timeline_assert_start_order() {
        let tracker = ExecutionTracker::new();
        let harness = TestHarness::new().with_tracker(tracker.clone());

        let dag = DagBuilder::new("order_test", "Order Test DAG")
            .add_task(tracked(SimpleTaskInner::new("first"), &tracker))
            .add_task_with_deps(
                tracked(SimpleTaskInner::new("second"), &tracker),
                &["first"],
            )
            .build()
            .unwrap();

        let _ = harness.execute(&dag).await;

        // This should pass
        harness
            .timeline()
            .assert_start_order(&["first", "second"])
            .await
            .unwrap();
    }

    #[tokio::test]
    async fn test_timeline_assert_started_before() {
        let tracker = ExecutionTracker::new();
        let harness = TestHarness::new().with_tracker(tracker.clone());

        let dag = DagBuilder::new("before_test", "Before Test DAG")
            .add_task(tracked(SimpleTaskInner::new("early"), &tracker))
            .add_task_with_deps(tracked(SimpleTaskInner::new("late"), &tracker), &["early"])
            .build()
            .unwrap();

        let _ = harness.execute(&dag).await;

        // This should pass
        harness
            .timeline()
            .assert_started_before("early", "late")
            .await
            .unwrap();

        // This should fail
        let result = harness
            .timeline()
            .assert_started_before("late", "early")
            .await;
        assert!(result.is_err());
    }

    #[tokio::test]
    async fn test_timeline_detects_concurrent_execution() {
        let tracker = ExecutionTracker::new();
        let harness = TestHarness::new()
            .with_tracker(tracker.clone())
            .with_concurrency(4);

        // A -> B, C (B and C should run concurrently after A)
        let dag = DagBuilder::new("concurrent_test", "Concurrent Test DAG")
            .add_task(tracked(SimpleTaskInner::new("a"), &tracker))
            .add_task_with_deps(
                tracked(SlowTaskInner::new("b", Duration::from_millis(50)), &tracker),
                &["a"],
            )
            .add_task_with_deps(
                tracked(SlowTaskInner::new("c", Duration::from_millis(50)), &tracker),
                &["a"],
            )
            .build()
            .unwrap();

        let _ = harness.execute(&dag).await;

        // B and C should have run concurrently
        assert!(harness.timeline().ran_concurrently("b", "c").await);
    }

    #[tokio::test]
    async fn test_timeline_entries_for_task() {
        let tracker = ExecutionTracker::new();
        let harness = TestHarness::new().with_tracker(tracker.clone());

        let dag = DagBuilder::new("entries_test", "Entries Test DAG")
            .add_task(tracked(SimpleTaskInner::new("target"), &tracker))
            .build()
            .unwrap();

        let _ = harness.execute(&dag).await;

        let entries = harness
            .timeline()
            .entries_for_task(&TaskId::new("target"))
            .await;

        // Should have Started and Completed events
        assert_eq!(entries.len(), 2);
        assert_eq!(entries[0].event_type, TimelineEventType::Started);
        assert_eq!(entries[1].event_type, TimelineEventType::Completed);
    }

    // ==========================================================================
    // Failure Injection Tests
    // ==========================================================================

    #[tokio::test]
    async fn test_failure_injection_config() {
        let injection = FailureInjection::new()
            .always_fail("task_a")
            .fail_with("task_b", "custom error")
            .fail_times("task_c", 3);

        assert!(injection.always_fail.contains(&"task_a".to_string()));
        assert_eq!(
            injection.fail_with_message.get("task_b"),
            Some(&"custom error".to_string())
        );
        assert_eq!(injection.fail_n_times.get("task_c"), Some(&3));
    }

    #[tokio::test]
    async fn test_harness_with_failure_injection() {
        let harness =
            TestHarness::new().with_failures(FailureInjection::new().always_fail("will_fail"));

        // The failure injection is configured but the harness doesn't
        // automatically intercept tasks - this test verifies config works
        assert!(
            harness
                .failure_injection
                .always_fail
                .contains(&"will_fail".to_string())
        );
    }
}
